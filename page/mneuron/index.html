<html>
<head>
    <link rel="stylesheet" href="index_style.css" type="text/css" />
    <script type="text/javascript" src="js/elevatezoom-master/jquery-1.8.3.min.js"></script> 
    <script src="js/elevatezoom-master/jquery.elevatezoom.js" type="text/javascript"></script> 
</head>

<body>


    <div class="content">
    <a href="https://github.com/donglaiw/mNeuron"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://camo.githubusercontent.com/567c3a48d796e2fc06ea80409cc9dd82bf714434/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f6c6566745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_left_darkblue_121621.png"></a>
    <h1>mNeuron: A Matlab Plugin to Visualize Neurons from Deep Models</h1>
    <p id="authors"> 
    <a href="http://people.csail.mit.edu/donglai">Donglai Wei</a>
    <a href="http://people.csail.mit.edu/bzhou/">Bolei Zhou</a> 
    <a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a>
    <a href="https://billf.mit.edu/">William T. Freeman</a><br>
    {donglai, bzhou}@csail.mit.edu   &nbsp;&nbsp;&nbsp; &nbsp;         {torralba, billf}@mit.edu<br/>
    Massachusetts Institute of Technology</p>
    <h3>Content:</h3>
    <ul><li style="margin: 0 50px 20px 0;"><a href="https://github.com/donglaiw/mNeuron">
    Code</a>: visualize neurons trained from deep learning packages through backpropagation optimization [support <a href="http://caffe.berkeleyvision.org/">Caffe</a> and <a href="http://www.vlfeat.org/matconvnet/">matconvnet</a>] </li>
        <li style="margin: 0 50px 20px 0;">Demo <br/><ul>
        <!--li>Demo <br/><ul style="list-style: none;"!-->
            <li><a href="#v_single">Visualize a single neuron</a></li>
            <li><font style="color:red;font-weight:bold;">Visualize a neuron pattern</font> &nbsp;<img width=5% src="logo/new.jpg"><ol>
            <li><a href="#v_intra">Intra-class variation</a>: optimize learned fc6 or fc7 relu mask</li>
            <li><a href="#v_bcode">Hierarchical binary CNN code</a>: optimize with predefined relu mask</li>
            <li><a href="#v_inpaint">Image completion with CNN</a>: optimize with boundary condition</li>
            </ol></li></ul>
    </li>
    <!--li><a href="http://arxiv.org/pdf/1507.02379.pdf">Documentation</a> (arxiv link)</li!-->
    <li>Reference
        <ul>
            <li>The visualization method is modified upon <i>"Mahendran and Vedaldi. Understanding Deep Image Representations by Inverting Them"</i> [<a href="http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf">pdf</a>, <a href="https://github.com/aravindhm/deep-goggle.git">code</a>]</li>
            <li>Documentation (<a href="data/cnn_visual_arxiv.pdf">draft</a>)</li>
        </ul>
    </li>
    </ul>
    <!--font size=2>*Demo 1-3: results are generated with AlexNet, but the toolbox supports all models</font!-->
    </div>'


    <div class="content" id="v_single">
        <h3>Demo 1: Visualize A Single Neuron <font size=2>(<a href="https://github.com/donglaiw/mNeuron/blob/master/V_neuronInv.m">V_neuronInv.m</a>)</font></h3>
     <b>Goal: </b> Find an image that optimize the activation of a single neuron [Erhan et al., Simonyan et al., Zhou et al.]  <br/><br/>
     <em>a. Different Layers</em>: (<a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet">AlexNet</a>) We visualize Conv1,3,5 neurons learned from ImageNet dataset. With the increasing layer depth, neurons are learned to recognize from simple edge/blob and texture pattern to complex object parts and class. (For Conv 5, we retrive real images for comparison with Zhou et al.)<br/>
     <div style="text-align:center;"><br/><img id="zoom_01" src="data/single_layer.png" width=90% /></div><br/><br/><br/>
     <em>b. Tessellation Art</em>: 
     <a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet">(GoogleNet</a>)
      Instead of optimizing the activation of a single neuron in 1x1 receptive field, we extend the receptive field to the full extent (14x14 for GoogleNet inception4a). Below, we manually organize the visualization results of some neurons, which tessellate the object parts into artistic wallpapers<br/>
     <div style="text-align:center;"><br/><img id="zoom_02" src="data/single_art_gn.png" width=90% /></div><br/><br/><br/>
     <em>c. Different Dataset</em>: (<a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet">AlexNet</a>) We visualize Conv4-5 neurons learned from two different datasets: ImageNet and Places. <br/>
     <div style="text-align:center;"><br/><img id="zoom_03" src="data/single_dataset.png" width=70% /></div><br/><br/><br/>
     <em>d. Different Networks</em>: 
     (<a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet">AlexNet</a>, 
     <a href="https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md">VGG-16</a>,
     <a href="https://gist.github.com/mavenlin/d802a5849de39225bcc6">NIN</a>,
     <a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet">GoogleNet</a>)
      Given four semantic patches "dog, fire, clock and wheel", we visualize the top-activated neurons in roughly aligned layers (right before Fc layers) from four different Networks.<br/>
     <div style="text-align:center;"><br/><img id="zoom_04" src="data/single_network_0710.png" width=90% /></div><br/><br/><br/>
     <a href="http://vision03.csail.mit.edu/cnn_art/conv_visual.html">More Results</a>
     </div>'


    <div class="content" id="v_intra">
        <h3><font color="red">Demo 2:</font> Visualize Intra-class Variation <!--font size=2>(<a href="">V_neuron_topic.m</a>)</font!--></h3>
        <b>Intra-class Variation: </b></font> 
        Given training images of an object class, we cluster Fc neuron responses to find common neuron firing patterns (neuron pathway). For each pattern, we choose top-k activated neurons and numerically find an image that optimizes the total activation of these neurons
        <div style="text-align:center;"><br/><img id="zoom_05" src="data/multi_intra.png" width=90% /></div>
     </div>'

     <div class="content" id="v_bcode">
     <h3><font color="red">Demo 3:</font> Visualize Binary CNN Code <!--font size=2>(<a href="">V_neuron_binarycode.m</a>)</font!--></h3>
     Given an input image, we can define its binary code as its relu masks m<sub>5-7</sub>. 
     [<a href="http://arxiv.org/abs/1407.1610">Agrawal et al.</a>] points out that these binary code achieve similar classification result
     as the original deep features.
     We here consider three different binary code with increasing length: m<sub>7</sub>, m<sub>6-7</sub>, m<sub>5-7</sub>.<br/><br/>
     To visualize the binary codes of an image, 
     we do the same "single-neuron visualization" for its Fc8 label, 
     except applying the additional binary code after the original relu layers during the optimization.
     With more layers of binary code, we can recover the essence of the orignial image, 
     which partially explain its effectiveness for classification.
     <div style="text-align:center;"><br/><img id="zoom_06" src="data/hashcode_fc.png" width=50% /></div>
     </div>'

     <div class="content" id="v_inpaint">
         <h3><font color="red">Demo 4:</font> Image Completion with CNN <font size=2>(<a href="https://github.com/donglaiw/mNeuron/blob/master/V_inpaint.m">V_inpaint.m</a>)</font></h3>
     Details see Sec 5.3 in the documentation. 
     The gist is that we can do the neuron visualization (either single neuron or neuron pattern) with boundary constraints on the image.
     <div style="text-align:center;"><br/><img id="zoom_07" src="data/ip3.png" width=70% /></div>
     </div>'


<script>
    for(var i=1;i<=7;i++){
        $("#zoom_0"+i).elevateZoom();
    }
</script>
 
 </body>
 </html>
