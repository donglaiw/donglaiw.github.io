<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>YouMVOS</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="We seek to understand the arrow of time in videos--what makes videos look like playing forwards or backwards?  Can we visualize the cues? Can the arrow of time be a supervisory signal useful for activity analysis? To this end, we apply a learning-based approach to a large set of videos. To learn the arrow of time efficiently and reliably, we design a ConvNet suitable for extended temporal footprints and for the class activation visualization, and study the effect of artificial cues, such as inematographic conventions, on learning. Our trained model achieves the state-of-the-art performance on two large-scale real-world video datasets.  Through cluster analysis, we examine the learned visual cues, showing when and where they occur. Lastly, we use the trained ConvNet for two applications: self-supervision for action recognition, and video forensics -- determining whether Hollywood film clips have been deliberately reversed in time as special effects.">
<meta name="keywords" content="arrow of time; unsupervised learning; deep learning;">

<!-- Fonts and stuff -->
<link href="./src/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./src/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./src/iconize.css">
<script async="" src="./src/prettify.js"></script>
<style>
.highlight {
  padding: 1.5rem;
  margin-right: 0;
  margin-left: 0;  
}

</style>

</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1>YouMVOS: An Actor-centric Multi-shot Video Object Segmentation Dataset</h1>

	<div class="authors">
	  <a href="https://donglaiw.github.io/">Donglai Wei</a><sup>1&dagger;</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Siddhant Kharbanda<sup>2&dagger;*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Sarthak Arora<sup>2*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Roshan Roy<sup>2*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Nishant Jain<sup>2*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Akash Palrecha<sup>2*</sup><br/>
	  Tanav Shah<sup>2*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Shray Mathur<sup>2*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Ritik Mathur<sup>2*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Abhijay Kemka<sup>2*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Anirudh Chakravarthy<sup>2*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Zudi Lin<sup>2</sup><br/>
	  Won-Dong Jang<sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Yansong Tang<sup>3,4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Song Bai<sup>5</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  James Tompkin<sup>6</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Philip H.S. Torr<sup>4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Hanspeter Pfister<sup>2</sup>
	</div>

	<div class="affiliations">
      1. Boston College&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  2. Harvard University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      3. Tsinghua-Berkeley Shenzhen Institute<br/>
	  4. University of Oxford&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      5. ByteDance Inc.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      6. Brown University
	</div>
       <div>&dagger; Equally contributed.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
       * Works are done as interns at Harvard University.</div>

	<div class="venue">Conference on Computer Vision and Pattern Recognition (<a href="http://cvpr2022.thecvf.com/" target="_blank">CVPR</a>) 2022 </div>
      
      </div>
      <center>
 [<a href="../../paper/2022_cvpr_youmvos.pdf">PDF</a>]
 [<a href="../../paper/2022_cvpr_youmvos_supp.pdf">Supplementary</a>]
[<a href="https://github.com/donglaiw/AoT_TCAM">PyTorch code</a>]
[<a href="https://drive.google.com/drive/folders/1XBgM-VrFfwOz7p1m5Ts2Vo81O7_qAXKy?usp=sharing">Dataset</a>]
      </center>
<br/>    
<br/>    
      <center>
          <img src="./src/teaser.png" border="0" width="95%">
      </center>
<div class="section abstract">
          <p style="margin-top: -1em;"> Multi-shot video object segmentation (MVOS). In multi-shot videos, MVOS aims to track and segment selected
recurring objects despite changes in appearance (e.g., person in green masks) and disconnected shots (e.g., person in red
masks). We show sample (a) frames, (b) segmentation masks, and (c) timelines for the Gangnam Style video in our dataset.</p>
          </div>

<div class="section abstract">
	<h2>Abstract</h2>
    <p>	Many video understanding tasks require analyzing multi-shot videos, but existing datasets for video object segmentation (VOS) only consider single-shot videos. To address this challenge, we collected a new dataset---YouMVOS---of <b>200</b> popular YouTube videos spanning <b>ten</b> genres, where each video is on average five minutes long and with <b>75</b> shots. We selected recurring actors and annotated <b>431K</b> segmentation masks at a frame rate of six, exceeding previous datasets in average video duration, object variation, and narrative structure complexity. We incorporated good practices of model architecture design, memory management, and multi-shot tracking into an existing video segmentation method to build competitive baseline methods. Through error analysis, we found that these baselines still fail to cope with cross-shot appearance variation on our YouMVOS dataset. Thus, our dataset poses new challenges in multi-shot segmentation towards better video analysis.
    </p>
      </div>

<div class="section demo">
	<h2>Example Video</h2>
	<br>
	<center>
      <center><img src="./src/dataset_genre.png" width="95%"></center>
      <!--iframe width="560" height="315" src="https://www.youtube.com/embed/1zfZhXkOzCw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe-->
	
	    </center>
    </div>
<br>
<div class="section materials">
<h2>Citation</h2>
<p>Bibilographic information for this work:</p>

<pre class="highlight" style="margin-top:-1.5em">
@inproceedings{wei2022youmvos,
  title={YouMVOS: An Actor-centric Multi-shot Video Object Segmentation Dataset},
  author={Wei, Donglai and Kharbanda, Siddhant and Arora, Sarthak and others},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2022}
}
</pre>
<h2>Acknowledgement</h2>
<p>
This work was supported by NSF grants NCS-FO-2124179, NIH grant R01HD104969, UKRI grant Turing AI Fellowship EP/W002981/1, and EPSRC/MURI grant EP/N019474/1. We also thank the Royal Academy of Engineering and FiveAI.
</div>

</div></div></body></html>
